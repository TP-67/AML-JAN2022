---
title: "Week7 Tutorial - Regularized Regression"
date: "13/3/2021"
output: html_notebook
---

```{r}
# Import libraries
library(tidyverse)
library(DataExplorer)
library(VIM)
library(mice)
library(missForest)
# Sampling, cross validation
library(caret)
library(psych)
library(reshape2)
library(ROCR)
library(naivebayes)
# Regression models
library(glmnet)
library(mlbench)
library(ggplot2)
# Plotting coeffs
library(broom)
```

## 1. Dataset Description

### 1.1 General Information
#### Data Set: https://www.kaggle.com/sajidsaifi/prostate-cancer
#### Reference: https://www.kaggle.com/smogomes/prostate-cancer-prediction-model/notebook
This dataset contains different measures of prostate cancer tissue. Our goal is to design a logistic regression model to predict whether an instance is malignant (M) cancer or benign (B) cancer.

- Feature number: 9
- Instance number: 100

### 1.2 Independent Variables
(1) Radius (Continuous variable).
(2) Texture (Continuous variable).
(3) Perimeter (Continuous variable).
(4) Area (Continuous variable).
(5) Smoothness (Continuous variable).
(6) Compactness (Continuous variable).
(7) Symmetry (Continuous variable).
(8) Fractal (Continuous variable).

### 1.3 Dependent Variables
(1) Diagnosis result (Categorical variable): M (1), B (0).

```{r}
# Load data
df <- read.csv('Prostate_Cancer.csv', header = T)
```

## 2. Dataset Overview
```{r}
summary(df)
```

```{r}
# Dataset info
str(df)
```

## 4. Data Imputation
```{r}
# Prior to treating missing values any blanks in the data set must be converted to NA.
df <- mutate_all(df, na_if, "")
```

### 4.1 Plot Missing Values
```{r}
# Viewing and counting missing data using DataExplorer package.
plot_missing(df)
```
Good News, there is no missing value now.

```{r}
# Using VIM library
vim_plot <- aggr(df, numbers=TRUE, prop = c(TRUE, FALSE))
```
As we can see, we have 100 records in total, and no missing value in this data set.

This function plots and calculates the amount of missing values in each variable and in certain combinations. On the right-hand chart, we can see the missing values' distribution in a combination. Notice that left-hand chart uses proportions  as Y-axis, while the right one uses real number because we set prop = c(TRUE, FALSE).

```{r}
# Using mice library
md.pattern(df)
```
This chart demonstrates the distribution of missing values. The values on the left side represent different groups of the data set. The values on the right side represent the total number of variables that contain missing value. The top line are names of different variables. The bottom line represents the number of missing values of each variable.

```{r}
# Overview of new data set
sum(is.na(df))
head(df, 10)
```

## 5. Data Analyze

### 5.1 Distribution of diagnosis results
```{r}
ggplot(df, aes(x=diagnosis_result), color="darkblue") +
  geom_histogram(stat='count', aes(y = ..count..), width=.1, fill="darkblue", alpha=0.4) +
  theme(axis.text.x=element_text(angle=0, hjust=1, vjust=.5))
```

### 5.2 Distribution of the dependent variable.
```{r}
ggplot(df, aes(y = diagnosis_result)) +
  geom_bar(width=.2, fill="darkblue", alpha=0.4) +
  xlab("Count") +
  ylab("Cancer") +
  geom_text(stat='count', aes(label=..count..), hjust=1.5, vjust=.5)
```

```{r}
barplot(table(df$diagnosis_result),
        xlab="Class (M = Benign, B = Malignant)", ylab="Count", col=c("darkblue","red"),
        legend = levels(df$diagnosis_result), beside=TRUE)
```

## 6. Feature Selection

### 6.1 Correlation matrix
```{r}
df.cor <- df %>%
      mutate(diagnosis_result = ifelse(diagnosis_result == "B", 0, 1))
str(df.cor)
```

```{r}
cormat <- round(cor(select(df.cor, c("diagnosis_result", "radius", "texture", "perimeter", "area", "smoothness", "compactness", "symmetry", "fractal_dimension"))), 3)
# Merge data
melted_cormat = melt(cormat)
```

```{r}
ggplot(data=melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() +
  theme(axis.text.x=element_text(angle=45, hjust=1, vjust=.5)) +
  geom_text(aes(Var2, Var1, label = value),color='white', size=3)
```
From this diagram, we can see that fractal dimension and texture have low correlations with other features. Therefore, we remove these two features from our database. In addition, we remove perimeter column from our dataset for avoiding duplication because it has a strong correlation with the area column.

```{r}
df.cor = subset(df.cor, select = -c(id, fractal_dimension, texture, perimeter) )
str(df.cor)
```

### 6.2 Check Gaussianness
```{r}
# Check if the density plot os Gaussian
plot_density(df)
```

## 7. Data Normalization
```{r}
# Normalization
min_max_norm <- function(x) {
      (x - min(x)) / (max(x) - min(x))
}
df.norm = df.cor
df.norm[2:6] <- as.data.frame(lapply(df.cor[2:6], min_max_norm))
```

#### Notice that we use Min-Max normalization instead of Standardization for data rescaling for two reasons.First, we want to bound the data between 0 and 1. Also, all weather data doesn't follow Gaussian Distribution. Their distributions are unkown to us.

```{r}
sum(is.na(df))
summary(df.norm)
```

```{r}
# df.norm = df.norm %>% mutate(value = 1)  %>% spread(diagnosis_result, value,  fill = 0 )
# head(df.norm)
```

## 8. Data Set Partitioning
We use 80% of the data set for training, and the rest for testing.

```{r}
# 80% of the sample size
smp_size <- floor(0.80 * nrow(df))
set.seed(123)
train_ind <- sample(seq_len(nrow(df.norm)), size=smp_size)
train <- df.norm[train_ind, ]
test <- df.norm[-train_ind, ]
```

## 9. Experiments

### 9.1 Baseline Linear regression

#### In regression with a single independent variable, the coefficient tells you how much the dependent variable is expected to increase (if the coefficient is positive) or decrease (if the coefficient is negative) when that independent variable increases by one.
(ref: https://dss.princeton.edu/online_help/analysis/interpreting_regression.htm)

```{r}
l_baseline <- glm(formula = diagnosis_result ~ . , data = train)
summary(l_baseline)
```

```{r}
names(l_baseline)
```

```{r}
coefficients(l_baseline)
```

```{r}
plot(coefficients(l_baseline))
```

### 9.2 Rigid regression
```{r}
# We convert the data into matrix because the model "glmnet" we are going to use data in matrix form.
# If your dataset contains categorical values, all variables need to be converted into numerical form before converting the dataset into a matrix.
x <- data.matrix(train[,-1])
y <- train$diagnosis_result
```

Glmnet parameter alpha=0 for ridge regression.
For numerical prediction choose 'family=gaussian', for classification 'family=binomial'.
```{r}
# glmnet by defaut chooses 100 lambda values that are data dependent
l_ridge <- glmnet(x, y, family="gaussian", alpha=0)

# Plot the coefficients with changing lambda
plot(l_ridge, xvar='lambda', label=T)
```
(1) All features are relevant to independent variables to some extend since the range of correlation is (-1.0, 1.2).
(2)From the plot note that the coefficients reduce when lambda increases. However all 13 attributes remain, none of them are dropped. 

#### Now we need to find the best value for lambda. This may be done using the built-in cross validation of cv.glmnet.
```{r}
# Now the question is on how to choose lambda value?
# The following plots MSE for various training and validation samples (concept to be covered in cross validation) and various lambdas.
cv_out_ridge = cv.glmnet(x, y, alpha=0)
plot (cv_out_ridge)
```

```{r}
names(cv_out_ridge)
```

#### Two lambda values may be noted. 'lambda.min', 'lambda.1se'- lambda for error within 1 standard deviation.
```{r}
# lambda_min gives us the lambda value with the lowest error.
lambda_min <- cv_out_ridge$lambda.min
lambda_min
```

```{r}
# lambda_1se gives us the lambda value with slightly higher error but better regularization.
lambda_1se<- cv_out_ridge$lambda.1se
lambda_1se
```

```{r}
log(lambda_1se)
```

#### Now let us plot the ridge regression output once again
```{r}
# By increasing the regularization, we reduce the weight value of some variables. Each variable has been given the new weight.
plot(l_ridge, xvar = 'lambda', label=T)
abline(v = log(cv_out_ridge$lambda.1se), col = "red", lty = "dashed")
abline(v = log(cv_out_ridge$lambda.min), col = "blue", lty = "dashed")
```

```{r}
# Now set lambda to one of these values and build the model.
l_ridge_final <- glmnet(x, y, family="gaussian", lambda = lambda_1se, alpha=0)
# We can see the correlations listed here.
coef(l_ridge_final)
```

```{r}
plot(coef(l_ridge_final))
```

```{r}
# Alternate plot of the coeffs
coef(l_ridge_final) %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  top_n(25, wt = abs(value)) %>%
  ggplot(aes(value, reorder(row, value))) +
  geom_point() +
  ggtitle("Top 25 influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```

```{r}
# Prediction with training set
p1 <- predict(l_ridge_final, x)
rmse_l_ridge_final <- sqrt(mean((train$diagnosis_result-p1)^2))
rmse_l_ridge_final
```

```{r}
# Repeat the same with lasso regression
cv_out_lasso = cv.glmnet(x, y, alpha = 1)
plot (cv_out_lasso)
```

```{r}
names(cv_out_lasso)
```

#### Two lambda values may be noted. 'lambda.min', 'lambda.1se'- lambda for error within 1 standard deviation.
```{r}
lambda_min <- cv_out_lasso$lambda.min
lambda_min
lambda_1se<- cv_out_lasso$lambda.1se
lambda_1se
log(lambda_1se)
```

```{r}
# Now let us plot the lasso regression output once again
l_lasso <- glmnet(x, y, family="gaussian", alpha=1)
plot(l_lasso, xvar = 'lambda', label=T)
abline(v = log(cv_out_lasso$lambda.1se), col = "red", lty = "dashed")
abline(v = log(cv_out_lasso$lambda.min), col = "blue", lty = "dashed")
```
When the lambda equals -7, we have 5 input variables. However, when lambda reaches 'min_lam', there are only three variables left. This means three of them have become a very low value of 0.

```{r}
# Now set lambda to one of these values and build the model
l_lasso_final <- glmnet(x, y, family="gaussian", lambda = lambda_1se, alpha=0)
coef(l_lasso_final)
plot(coef(l_lasso_final))
```

```{r}
# Finally predict using train and test set and compare the two models.
cbind(coefficients(l_baseline), coef(l_ridge_final), coef(l_lasso_final) )
```
These three columns are baseline, rigid, and lasso, respectively.

```{r}
cat("Last Block!")
```
