---
title: "Week4 Tutorial - Data Preprocessing"
date: "21/2/2021"
output: html_notebook
---

```{r}
# Import libraries
library(tidyverse)
library(DataExplorer)
library(VIM)
library(mice)
library(missForest)
# Impute using caret preprocess function
library(caret)
library(psych)
```

## Data set description
#### Reference: https://www.kaggle.com/arashnic/big-mart-sale-forecast
(1) BigMart Sale Forecast dataset contains 1559 products across 10 stores in 10 different cities.
(2) Each product and store have several pre-defined features.
(3)The goal is to build a predictive model to predict the sales of each product in the future.
(4)The purpose of this project is that BigMart tries to understand different roles which each product and store play.

## Feature description
### Independent variables
(1) Item_Identifier: Product ID.
(2) Item_Weight: Weight of products (continuous variable).
(3) ItemFatContent: Binary indicator (low fat or not, categorical variable).
(4) Item_Visibility: The ratio of the area of displaying a particular product to the total area of a store (in percentage, continuous variable).
(5) Item_Type: Category (daily, foods, others, ..., categorical variable).
(6) Item_MRP: Maximum retail price of the product (continuous variable).
(7) Outlet_Identifier: Store ID.
(8) OutletEstablishmentYear: The year in which store was established (continuous variable).
(9) Outlet_Size: Store's ground area (categorical variable).
(10) Outlet_Location_Type: The type of city in which the store is located (categorical variable).
(11) Outlet_Type: Store type (categorical variable).

### Dependent variable
(1) ItemOutletSales: Sales of the product in a particular store (continuous variable). The variable to be predicted.

```{r}
# Load data
df <- read.csv('train.csv', header = T)
df1 <- read.csv('train.csv', header = T)
df2 <- read.csv('train.csv', header = T)
```

## Selected Data Preprocessing Methods
### 1. Missing value treatement
### 2. Data type conversion
### 3. Data Encoding
### 4. Data Normalization

## 1. Missing Value Treatment - Imputation
```{r}
# We use factors for categorical data. Each label has a unique interger as factor.
# When dataframe is loaded, if the 'factor' variables are read  as 'char', you need to convert them to type "factor" with the following:
df$Item_Fat_Content <- as.factor(df$Item_Fat_Content)
df$Item_Type <- as.factor(df$Item_Type)
df$Outlet_Size <- as.factor(df$Outlet_Size)
df$Outlet_Location_Type <- as.factor(df$Outlet_Location_Type)
df$Outlet_Type <- as.factor(df$Outlet_Type)

# Now we convert 'chr' into 'factor'.
str(df)
```

```{r}
# Prior to treating missing values any blanks in the dataset must be converted to NA
df <- mutate_all(df, na_if, "")
df1 <- mutate_all(df1, na_if, "")
df2 <- mutate_all(df2, na_if, "")
# View (df)
```

```{r}
# If your dataset has no missing values, introduce missing values to illustrate your abilities in imputation.
# df <- prodNA(ds, noNA = 0.1) # introduces 10% missing values at random
```

```{r}
# Viewing and counting missing data
# Uses DataExplorer package
plot_missing(df)
```

We can see that only two features (Item_Weight and Outlet_Size) have missing values. The proportion of missing values for each feature is 17.17% and 28.28%, respectively.

```{r}
# Needs library(VIM)
vim_plot <- aggr(df, numbers=TRUE, prop = c(TRUE, FALSE))
```

This function plots and calculates the amount of missing values in each variable and in certain combinations. From the left-hand chart, we can see that Item_Weight and Outlet_Size are missing 17% and 28% values, respectively. On the right-hand chart, we can see the missing values' distribution in a combination. Notice that left-hand chart uses proportions  as Y-axis, while the right one uses real number because we set prop = c(TRUE, FALSE).

```{r}
# Uses mice package
md.pattern(df)

# Explain the plots using references on mice and VIM (Ref: https://www.datacamp.com/community/tutorials/visualize-data-vim-package)
```

This chart demonstrates the distributon of missing values. The values on the left side represent different groups of the dataset. The values on the right side represent the total number of variables that contain missing value. The top line are names of different variables. The bottom line represents the number of missing values of each variable. From this chart, we can see that there are two variables that have missing values. The number of missing points are 1463 and 2410, respectively. Furthermore, there is no overlap area between two variables' missing parts.

### 1.1.1 Imputation for continuous variables Method 1
```{r}
# Reference: https://www.codingprof.com/how-to-replace-nas-with-the-mode-most-frequent-value-in-r/
# Imputing missing values in continuous variables using mean or median.
head(df, 10)
df$Item_Weight = ifelse(is.na(df$Item_Weight),
                     ave(df$Item_Weight, FUN = function(x) mean(x, na.rm = TRUE)),
                     df$Item_Weight)
head(df, 10)

# df$Salary = ifelse(is.na(df$Salary),
#                         ave(df$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
#                         df$Salary)
```

### 1.1.2 Imputation for continuous variables Method 2
```{r}
# Imputing missing values of the continuous variable using median.
# This function only impute continuous variables.
head(df1, 10)
preProcValues <- preProcess(df1, method = "medianImpute")
df1 <- predict(preProcValues, df1)
head(df1, 10)
```

### 1.2.1 Imputation for categorical variables Method 1
```{r}
# How would you complete imputing categorical values in this dataset? 
# Imputing missing values in categorical variables using mode.

# 1) Create a function that returns mode of a variable.
fun_mode <- function(x) {
  # List unique variables.
  unique_val <- unique(x)

  # Count occurrence of each variable.
  unique_num <- tabulate(match(x, unique_val))
  
  # Return the most frequent value.
  unique_val[which.max(unique_num)]
}

# 2) Replace NAs with the most frequent value.
# Specify the name of the column that have NAs. Then, use if_else() to find the missing values. Finally, replace NAs with a pre-defined function.
# We must convert chr to factor for Outlet_Size here, because factor variables are not imputable.
head(df1, 10)
df1$Outlet_Size = ifelse(is.na(df1$Outlet_Size),
                     fun_mode(df1$Outlet_Size),
                     df1$Outlet_Size)
head(df1, 10)
```

### 1.2.2 Imputation for categorical variables Method 2
```{r}
# Create a function for mode
Mode <- function(x) {
      ux <- sort(unique(x))
      ux[which.max(tabulate(match(x, ux)))]
}

head(df, 10)
df["Outlet_Size"] <- lapply(df["Outlet_Size"], function(x)
              replace(x, is.na(x), Mode(x[!is.na(x)])))
head(df, 10)
```

```{r}
# Check if there is any NAs
sum(is.na(df))
```

From the results, we can see that there is no missing values in our dataset, we are ready to go.

```{r}
# Checking frequencies of factor variables
table (df$Outlet_Size)
```

```{r}
# identify non numeric columns
i1 <- !sapply(df, is.numeric)
i1
```

### 1.3 Imputation Method 3 - Mice
```{r}
# Imputing missing values using mice for continuous variables
# Ref: https://rforpoliticalscience.com/2020/07/28/impute-missing-values-with-mice-package-in-r/
head(df2, 10)
imputed_df <- mice(df2, m=3)
df2 <- complete(imputed_df)
head(df2, 10)
```

This function only deal with continuous variables, leaving categorical variables as NAs still.

### 1.4 Imputation Method 4 - missForest
```{r}
# # Imputing missing values using missForest
# index <- c(2, 3)
# dr <- missForest(df2[2, 3], verbose = TRUE) # Is dr a dataframe?
# dq<- dr$ximp # What is dq?
# # View dq and comment
# # If there are any floating values where you expect an integer, round it to integer
# # you may use: dataset$col <- round (dataset$col)
# # Once again View the data and comment
# View(dr)
# View(dq)
```

## 2. Data type Conversion
```{r}
# where necessary use the following:
# df$col <- as.factor (df$col)
# df$col <- as.numeric (df$col)
# df$col <- as.character (df$col)
```

## 3. Data Encoding

### 3.1 Data Encoding - Converting to numeric - Ordinal/Nominal variables
```{r}
head(df1)
df1$Outlet_Size <- factor(df1$Outlet_Size,
                          levels = c('High', 'Medium', 'SmallFrance'),
                          labels = c(1, 2, 3))
head(df1)
```

```{r}
# This code is for strings
head(df1)
df1 <- df1 %>% mutate(Item_Fat_Content =
                     case_when(Item_Fat_Content == "LF" ~ "1", 
                               Item_Fat_Content == "low fat" ~ "2",
                               Item_Fat_Content == "Low Fat" ~ "3",
                               Item_Fat_Content == "reg" ~ "4",
                               Item_Fat_Content == "Regular" ~ "5"))

df1$Item_Fat_Content <- as.numeric(df1$Item_Fat_Content)
head(df1)
```

### 3.2 Onehot Encoding - Nominal variables

#### 3.2.1 Method 1 - Direct conversion from label to oneHot
```{r}
# Ref: https://cran.r-project.org/web/packages/DataExplorer/vignettes/dataexplorer-intro.html#correlation-analysis
head(df)
df = df %>% mutate(value = 1)  %>% spread(Item_Fat_Content, value,  fill = 0)
df = df %>% mutate(value = 1)  %>% spread(Item_Type, value,  fill = 0)
df = df %>% mutate(value = 1)  %>% spread(Outlet_Size, value,  fill = 0)
df = df %>% mutate(value = 1)  %>% spread(Outlet_Location_Type, value,  fill = 0)
df = df %>% mutate(value = 1)  %>% spread(Outlet_Type, value,  fill = 0)
head(df)

# dataset_dummified <- dummify(df_test, maxcat = 12L)
# View(dataset_dummified)
```

#### 3.2.2 Method 2 - Direct conversion from label to oneHot
```{r}
# # Given below are only sample codes. 
# # You need to change the datset reference in the following code to match with dataframes used earlier in this document
# 
# dmy <- dummyVars(" ~ .", data = df)
# dataset_onehot <- data.frame(predict(dmy, newdata = df))
# dataset_onehot
```

## 4. Data Normalization - suitable for continuous variables

### 4.1 Min-max normalization 
```{r}
# df$Salary <- (df$Salary - min(df$Salary))/(max(df$Salary) - min(df$Salary))
```

```{r}
# df$Salary <- round(df$Salary, digits = 3)
```

### Task - Write a code to perform min-max normalization on all the continuous variables in the datset
```{r}
summary(df)
min_max_norm <- function(x) {
      (x - min(x)) / (max(x) - min(x))
}
index <- c(2, 4, 7)
df[index] <- as.data.frame(lapply(df[index], min_max_norm))
summary(df)
```

### 4.2 Standardization
```{r}
describe(df1)
index <- c(2, 4, 7)
df1[index] <- as.data.frame(lapply(df1[index], function(x) if(is.numeric(x)){
  (x-mean(x))/sd(x)
} else x))
describe(df1)
```

### Task - Reference on other types of normalization

#### Reference: https://medium.com/swlh/data-normalisation-with-r-6ef1d1947970
In data science, we use data normalization to adjust the scales of data into a standard scale. For some of machine learning methods like, KNN, SVM, and BP(back-propagation), the algorithms computer distance between data points to improve the performances. Therefore, if variables are not in standard scales, some variables will have more influence to the final result, leading to bias in data. The larger scale values will influence smaller scale values. Especially in the gradient descent method, parameters will descend faster in a lower scale variable as its LR(learning rate) is higher.

#### Z-score Normalization(Standardization)
The results' mean and standard deviation of standardization will be 0, and 1 respectively.

\begin{equation}
\hat{x} = \frac{x - \mu}{\sigma} \label{eq:stand}
\end{equation}

Where $\mu$ represents mean, and $\sigma$ represents standard deviation of the dataset.

#### Robost Scalar
\begin{equation}
\hat{x} = \frac{x - median(x)}{(Q3 - Q1)} \label{eq:robost_norm}
\end{equation}

Where $median(x)$ represents median of the dataset. $Q3$ and $Q1$ represent 3rd quartile and 1st quartile respectively.

#### Min-Max Normalization
Min-Max Normalization rescale $x$ into a range of [0, 1] (or [-1, 1] if there is any negative values).

\begin{equation}
\hat{x} = \frac{x - min(x)}{max(x) - min(x)} \label{eq:min_max_norm}
\end{equation}

As we can see from the result, the largest value get normalized to 1, and the smallest get normalized to 0. The remaining values are in the range of (0, 1).

We can also rescale the dataset into any arbitrary range [a, b] by adding two hyper-parameters.

\begin{equation}
\hat{x} = a + \frac{(x - min(x))(b - a)}{max(x) - min(x)} \label{eq:min_max_a_b_norm}
\end{equation}

#### Mean Normalization

\begin{equation}
\hat{x} = \frac{x - \mu}{max(x) - min(x)} \label{eq:mean_norm}
\end{equation}

#### Unit Length

\begin{equation}
\hat{x} = \frac{x}{||x||)} \label{eq:unit_length_norm}
\end{equation}

Min-Max normalization and unit length normalization both rescale dataset into the range of [0, 1]. However, they will both be affected by outliers. Thus, it's better to use robost normalization when dealing with outliers.

#### Normalization and standardization
Min-Max Norm and Standardization are two most commonly used methods. Min-Max Norm bounds the data between 0 and 1, and Standardization doesn't have any boundaries.

So, how to choose between normalization and standardization?

|     | Need Boundary | Unknown Data Distribution |Gaussian Distribution | Data needs to be N($\mu$, 1) |
|  ----  | ----  | ---- | ---- | ---- |
| Min-Max-Normalization | v |  v | | |
| Standardization |  |  | v | v |
